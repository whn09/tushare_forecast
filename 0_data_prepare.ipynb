{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mobile-labor",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tushare # -i https://opentuna.cn/pypi/web/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrong-relaxation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tushare as ts\n",
    "\n",
    "import datetime\n",
    "from datetime import date\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "import boto3\n",
    "import sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dress-preparation",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts.set_token('1a1754d406d84b97ebb678b3cae9bfe3cbfaf4c0770f5409ae6e03b5')\n",
    "\n",
    "pro = ts.pro_api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lightweight-haven",
   "metadata": {},
   "outputs": [],
   "source": [
    "#查询当前所有正常上市交易的股票列表\n",
    "\n",
    "data = pro.stock_basic(exchange='', list_status='L', fields='ts_code,symbol,name,area,industry,list_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mathematical-brooklyn",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['list_date'] = pd.to_datetime(data['list_date'], format='%Y%m%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "labeled-thomas",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "voluntary-template",
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_data = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "natural-albania",
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_data.to_csv('stock_basic.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hawaiian-candy",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "studied-vegetable",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "green-plenty",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = '20200101'\n",
    "end_time = date.today().strftime(\"%Y%m%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smooth-rover",
   "metadata": {},
   "outputs": [],
   "source": [
    "alldata = None\n",
    "\n",
    "def get_daily(ts_code, start_date, end_date):\n",
    "    global alldata\n",
    "    df = pro.daily(ts_code=ts_code, start_date=start_date, end_date=end_date)\n",
    "    if alldata is None:\n",
    "        alldata = df\n",
    "    else:\n",
    "        alldata = pd.concat((alldata, df), axis=0)\n",
    "\n",
    "_ = data['ts_code'].apply(lambda x: get_daily(x, start_time, end_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greatest-match",
   "metadata": {},
   "outputs": [],
   "source": [
    "alldata['trade_date'] = pd.to_datetime(alldata['trade_date'], format='%Y%m%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seventh-softball",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_day(ts_code, trade_date):\n",
    "    list_date = data[data['ts_code'] == ts_code]['list_date']\n",
    "    if list_date.shape[0] > 0:\n",
    "        list_date = list_date.iloc[0]\n",
    "    else:\n",
    "        return None\n",
    "    list_day = (trade_date-list_date).days\n",
    "    return list_day\n",
    "\n",
    "alldata['list_day'] = alldata.apply(lambda x: get_list_day(x['ts_code'], x['trade_date']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recent-europe",
   "metadata": {},
   "outputs": [],
   "source": [
    "alldata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secure-commerce",
   "metadata": {},
   "outputs": [],
   "source": [
    "alldata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compound-championship",
   "metadata": {},
   "outputs": [],
   "source": [
    "alldata.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "starting-vermont",
   "metadata": {},
   "outputs": [],
   "source": [
    "alldata.to_csv(start_time+'_'+end_time+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "public-pierce",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = '1D'\n",
    "prediction_length = 7\n",
    "context_length = 365\n",
    "\n",
    "id_feature = 'ts_code'\n",
    "label_feature = 'close'\n",
    "time_feature = 'trade_date'\n",
    "sparse_features = ['area', 'industry']\n",
    "dynamic_dense_features = ['list_day']\n",
    "\n",
    "start_time = alldata[time_feature].min()\n",
    "end_time = alldata[time_feature].max()\n",
    "print('start_time:', start_time)\n",
    "print('end_time:', end_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hawaiian-exposure",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sparse_feature in sparse_features:\n",
    "    print(sparse_feature+':', len(data[sparse_feature].unique()), data[sparse_feature].unique()[:5], '... na:', sum(data[sparse_feature].isna()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "representative-gospel",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ids = []\n",
    "data_group = alldata.groupby(id_feature)\n",
    "cnt = 0\n",
    "for name, group in data_group:\n",
    "    if cnt % 1000 == 0:\n",
    "        print('cnt:', cnt)\n",
    "    cnt += 1\n",
    "    # print(name)\n",
    "    # print(group)\n",
    "    new_name = str(name)\n",
    "    # print(new_name)\n",
    "    ids.append(new_name)\n",
    "\n",
    "num_timeseries = len(ids)\n",
    "print('num_timeseries:', num_timeseries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incident-humor",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "younger-intermediate",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timeseries(df, dense_feature):\n",
    "    df_group = df.groupby(id_feature)\n",
    "    dense_df = pd.DataFrame({time_feature: [start_time, end_time]})\n",
    "    dense_df.set_index(time_feature, inplace=True)\n",
    "    dense_df = dense_df.resample(freq).asfreq()\n",
    "    # print(dense_df)\n",
    "    for name, group in df_group:\n",
    "#         print(name)\n",
    "        tmp_df = pd.DataFrame({name: group[dense_feature], time_feature:group[time_feature]})\n",
    "        tmp_df.set_index(time_feature, inplace=True)\n",
    "        if dense_feature == label_feature:\n",
    "            tmp_df = tmp_df.resample(freq).sum()  # aggregate\n",
    "        else:\n",
    "            tmp_df = tmp_df.resample(freq).mean()  # aggregate\n",
    "        # print(tmp_df)\n",
    "        dense_df = dense_df.join(tmp_df)\n",
    "    if dense_feature == label_feature:\n",
    "        dense_df = dense_df.resample(freq).sum()  # aggregate\n",
    "    else:\n",
    "        dense_df = dense_df.resample(freq).mean()  # aggregate\n",
    "        # TODO fill NaN\n",
    "        dense_df = dense_df.replace([np.inf, -np.inf], np.nan)\n",
    "        dense_df.fillna(method='ffill', inplace=True)\n",
    "        dense_df.fillna(method='bfill', inplace=True)\n",
    "        dense_df.fillna(0, inplace=True)\n",
    "    print('dense_df.shape:', dense_df.shape)\n",
    "    \n",
    "    timeseries = []\n",
    "    for i in range(num_timeseries):\n",
    "        dfi = dense_df.iloc[:,i]\n",
    "        timeseries.append(dfi)\n",
    "    # print(timeseries)\n",
    "    return timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competitive-barrier",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries = get_timeseries(alldata, label_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atmospheric-birmingham",
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_dense_timeseries = []\n",
    "for dense_feature in dynamic_dense_features:\n",
    "    print(dense_feature)\n",
    "    dense_timeseries = get_timeseries(alldata, dense_feature)\n",
    "    dynamic_dense_timeseries.append(dense_timeseries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lasting-europe",
   "metadata": {},
   "outputs": [],
   "source": [
    "property_cats = []\n",
    "\n",
    "ids_df = pd.DataFrame({id_feature: ids})\n",
    "\n",
    "for sparse_feature in sparse_features:\n",
    "    le = LabelEncoder()\n",
    "    new_data = ids_df.merge(data, how='left', on=id_feature)\n",
    "#     print(new_data)\n",
    "    features_arr = le.fit_transform(new_data[sparse_feature])\n",
    "    property_cats.append(features_arr.tolist())\n",
    "    le_classes = le.classes_.tolist()\n",
    "    print(sparse_feature, 'features_arr:', len(le_classes))\n",
    "    pickle.dump(le, open((sparse_feature+'_le.pickle').replace('/', '_'), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "soviet-hudson",
   "metadata": {},
   "outputs": [],
   "source": [
    "property_cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revised-method",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATETIME_START_OF_TRAIN = start_time\n",
    "DATETIME_END_OF_TRAIN = end_time+datetime.timedelta(days=1)-datetime.timedelta(days=2*prediction_length)\n",
    "DATETIME_START_OF_TEST = DATETIME_END_OF_TRAIN\n",
    "DATETIME_END_OF_TEST = end_time+datetime.timedelta(days=1)-datetime.timedelta(days=prediction_length)\n",
    "DATETIME_START_OF_PREDICT = DATETIME_END_OF_TEST\n",
    "DATETIME_END_OF_PREDICT = end_time+datetime.timedelta(days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "involved-latter",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_dataset = pd.Timestamp(DATETIME_START_OF_TRAIN, freq=freq)\n",
    "end_training = pd.Timestamp(DATETIME_END_OF_TRAIN, freq=freq)\n",
    "start_test = pd.Timestamp(DATETIME_START_OF_TEST, freq=freq)\n",
    "end_test = pd.Timestamp(DATETIME_END_OF_TEST, freq=freq)\n",
    "start_predict = pd.Timestamp(DATETIME_START_OF_PREDICT, freq=freq)\n",
    "end_predict = pd.Timestamp(DATETIME_END_OF_PREDICT, freq=freq)\n",
    "print('start_dataset:', start_dataset)\n",
    "print('end_training:', end_training)\n",
    "print('start_test:', start_test)\n",
    "print('end_test:', end_test)\n",
    "print('start_predict:', start_predict)\n",
    "print('end_predict:', end_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "asian-deficit",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = [\n",
    "    {\n",
    "        \"start\": str(timeseries[i].index[0]),\n",
    "        \"target\": timeseries[i][start_dataset:end_training][:-1].tolist(),  # We use -1, because pandas indexing includes the upper bound \n",
    "        \"dynamic_feat\": [dense_timeseries[i][start_dataset:end_training][:-1].tolist() for dense_timeseries in dynamic_dense_timeseries],\n",
    "        \"cat\": [property_cat[i] for property_cat in property_cats],\n",
    "        \"id\": ids[i]\n",
    "    }\n",
    "    for i in range(num_timeseries)\n",
    "]\n",
    "print(len(training_data), len(timeseries[0][start_dataset:end_training][:-1].tolist()), len(dense_timeseries[0][start_dataset:end_training][:-1].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brave-driver",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = [\n",
    "    {\n",
    "        \"start\": str(timeseries[i].index[0]),\n",
    "        \"target\": timeseries[i][start_dataset:end_test][:-1].tolist(),\n",
    "        \"dynamic_feat\": [dense_timeseries[i][start_dataset:end_test][:-1].tolist() for dense_timeseries in dynamic_dense_timeseries],\n",
    "        \"cat\": [property_cat[i] for property_cat in property_cats],\n",
    "        \"id\": ids[i]\n",
    "    }\n",
    "    for i in range(num_timeseries)\n",
    "]\n",
    "print(len(test_data), len(timeseries[0][start_dataset:end_test][:-1].tolist()), len(dense_timeseries[0][start_dataset:end_test][:-1].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organizational-particular",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_data = [\n",
    "    {\n",
    "        \"start\": str(timeseries[i].index[0]),\n",
    "        \"target\": timeseries[i][start_dataset:end_predict].tolist(),\n",
    "        \"dynamic_feat\": [dense_timeseries[i][start_dataset:end_predict].tolist() for dense_timeseries in dynamic_dense_timeseries],\n",
    "        \"cat\": [property_cat[i] for property_cat in property_cats],\n",
    "        \"id\": ids[i]\n",
    "    }\n",
    "    for i in range(num_timeseries)\n",
    "]\n",
    "print(len(predict_data), len(timeseries[0][start_dataset:end_predict].tolist()), len(dense_timeseries[0][start_dataset:end_predict].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "systematic-scout",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dicts_to_file(path, data):\n",
    "    with open(path, 'wb') as fp:\n",
    "        for d in data:\n",
    "            fp.write(json.dumps(d).replace('NaN', '\"NaN\"').encode(\"utf-8\"))\n",
    "            fp.write(\"\\n\".encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eight-winner",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "write_dicts_to_file(\"train_\"+freq+\".json\", training_data)\n",
    "write_dicts_to_file(\"test_\"+freq+\".json\", test_data)\n",
    "write_dicts_to_file(\"predict_\"+freq+\".json\", predict_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "municipal-medium",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "def copy_to_s3(local_file, s3_path, override=False):\n",
    "    assert s3_path.startswith('s3://')\n",
    "    split = s3_path.split('/')\n",
    "    bucket = split[2]\n",
    "    path = '/'.join(split[3:])\n",
    "    buk = s3.Bucket(bucket)\n",
    "    \n",
    "    if len(list(buk.objects.filter(Prefix=path))) > 0:\n",
    "        if not override:\n",
    "            print('File s3://{}/{} already exists.\\nSet override to upload anyway.\\n'.format(s3_bucket, s3_path))\n",
    "            return\n",
    "        else:\n",
    "            print('Overwriting existing file')\n",
    "    with open(local_file, 'rb') as data:\n",
    "        print('Uploading file to {}'.format(s3_path))\n",
    "        buk.put_object(Key=path, Body=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaging-controversy",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()             # IAM role to use by SageMaker\n",
    "region = sagemaker_session.boto_region_name\n",
    "\n",
    "s3_bucket = sagemaker_session.default_bucket()  # replace with an existing bucket if needed\n",
    "s3_prefix = 'time_series_forecast'    # prefix used for all data stored within the bucket\n",
    "s3_data_path = \"s3://{}/{}/data\".format(s3_bucket, s3_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handed-climb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "copy_to_s3(\"train_\"+freq+\".json\", s3_data_path + \"/train/train_\"+freq+\".json\", override=True)\n",
    "copy_to_s3(\"test_\"+freq+\".json\", s3_data_path + \"/test/test_\"+freq+\".json\", override=True)\n",
    "copy_to_s3(\"predict_\"+freq+\".json\", s3_data_path + \"/predict/predict_\"+freq+\".json\", override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "placed-marble",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
