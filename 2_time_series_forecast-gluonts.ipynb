{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install gluonts # -i https://opentuna.cn/pypi/web/simple # gluonts[Prophet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!sudo yum install -y R readline-devel\n",
    "\n",
    "# nnfor is optional\n",
    "# !sudo R -e 'install.packages(\"remotes\", repos=\"https://cloud.r-project.org\")'\n",
    "# !sudo R -e 'library(remotes) ; install_github(\"cran/plotrix\")'\n",
    "# !sudo R -e 'library(remotes) ; install_github(\"cran/glmnet\")'\n",
    "# !sudo R -e 'install.packages(c(\"nnfor\"), repos=\"https://cloud.r-project.org\", dependencies=TRUE)'\n",
    "\n",
    "!sudo R -e 'install.packages(c(\"forecast\", \"nnfor\"), repos=\"https://cloud.r-project.org\", dependencies=TRUE)'\n",
    "\n",
    "!pip install 'rpy2>=2.9.*,<3.*' -i https://opentuna.cn/pypi/web/simple  # Notice: gluonts need rpy2>=2.9.*,<3.*\n",
    "# or use conda\n",
    "#!conda install -c r rpy2 --yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install pystan==2.14 -i https://opentuna.cn/pypi/web/simple  # LunarCalendar\n",
    "!pip install fbprophet -i https://opentuna.cn/pypi/web/simple\n",
    "# or below is better\n",
    "# !conda install -c plotly plotly==3.10.0 --yes\n",
    "# !conda install -c conda-forge fbprophet --yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import mxnet as mx\n",
    "from mxnet import gluon\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = '1D'\n",
    "prediction_length = 7\n",
    "context_length = 365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(filename):\n",
    "    data = []\n",
    "    with open(filename, 'r') as fin:\n",
    "        while True:\n",
    "            line = fin.readline()\n",
    "            if not line:\n",
    "                break\n",
    "            datai = json.loads(line)\n",
    "            data.append(datai)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = load_json('train_'+freq+'.json')\n",
    "test = load_json('test_'+freq+'.json')\n",
    "predict = load_json('predict_'+freq+'.json')\n",
    "\n",
    "print(len(train[0]['target']), len(test[0]['target']), len(predict[0]['target']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_timeseries = len(train)\n",
    "print(num_timeseries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.dataset.common import ListDataset\n",
    "from gluonts.dataset.field_names import FieldName\n",
    "from gluonts.dataset.util import to_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_list = []\n",
    "for t in predict:\n",
    "    if len(t['target'])>=prediction_length:\n",
    "        predict_list.append({FieldName.TARGET: t['target'], FieldName.FEAT_STATIC_CAT: t['cat'], FieldName.FEAT_DYNAMIC_REAL: t['dynamic_feat'], FieldName.START: t['start'], FieldName.ITEM_ID: t['id']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ListDataset([{FieldName.TARGET: t['target'], FieldName.FEAT_STATIC_CAT: t['cat'], FieldName.FEAT_DYNAMIC_REAL: t['dynamic_feat'], FieldName.START: t['start'], FieldName.ITEM_ID: t['id']}\n",
    "                        for t in train], freq=freq)\n",
    "test_ds = ListDataset([{FieldName.TARGET: t['target'], FieldName.FEAT_STATIC_CAT: t['cat'], FieldName.FEAT_DYNAMIC_REAL: t['dynamic_feat'], FieldName.START: t['start'], FieldName.ITEM_ID: t['id']}\n",
    "                        for t in test], freq=freq)\n",
    "predict_ds = ListDataset(predict_list, freq=freq)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.dataset.multivariate_grouper import MultivariateGrouper\n",
    "grouper_train = MultivariateGrouper(max_target_dim=96)\n",
    "train_ds_multi = grouper_train(train_ds)\n",
    "test_ds_multi = grouper_train(test_ds)\n",
    "predict_ds_multi = grouper_train(predict_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_entry = next(iter(train_ds))\n",
    "print(train_entry.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_entry = next(iter(test_ds))\n",
    "print(test_entry.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_entry = next(iter(predict_ds))\n",
    "print(predict_entry.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_pandas_extend(instance: dict, feat_name: str = 'target', feat_index: int = 0, freq: str = None) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Transform a dictionary into a pandas.Series object, using its\n",
    "    \"start\" and \"target\" fields.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    instance\n",
    "        Dictionary containing the time series data.\n",
    "    freq\n",
    "        Frequency to use in the pandas.Series index.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.Series\n",
    "        Pandas time series object.\n",
    "    \"\"\"\n",
    "    if feat_name == 'target':\n",
    "        target = instance[feat_name]\n",
    "    else:\n",
    "        target = instance[feat_name][feat_index]\n",
    "    start = instance[\"start\"]\n",
    "    if not freq:\n",
    "        freq = start.freqstr\n",
    "    index = pd.date_range(start=start, periods=len(target), freq=freq)\n",
    "    return pd.Series(target, index=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_series = to_pandas(train_entry)\n",
    "train_series.plot()\n",
    "for i in range(len(train_entry['feat_dynamic_real'])):\n",
    "    train_dynamic_series = to_pandas_extend(train_entry, 'feat_dynamic_real', i)\n",
    "    train_dynamic_series.plot()\n",
    "plt.grid(which=\"both\")\n",
    "plt.legend([\"train series\"]+['dynamic_feat_'+str(i) for i in range(len(train_entry['feat_dynamic_real']))], loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_series = to_pandas(test_entry)\n",
    "test_series.plot()\n",
    "plt.axvline(train_series.index[-1], color='r') # end of train dataset\n",
    "for i in range(len(test_entry['feat_dynamic_real'])):\n",
    "    test_dynamic_series = to_pandas_extend(test_entry, 'feat_dynamic_real', i)\n",
    "    test_dynamic_series.plot()\n",
    "plt.grid(which=\"both\")\n",
    "plt.legend([\"test series\", \"end of train series\"]+['dynamic_feat_'+str(i) for i in range(len(test_entry['feat_dynamic_real']))], loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_series = to_pandas(predict_entry)\n",
    "predict_series.plot()\n",
    "plt.axvline(train_series.index[-1], color='r') # end of train dataset\n",
    "plt.axvline(test_series.index[-1], color='g') # end of test dataset\n",
    "for i in range(len(predict_entry['feat_dynamic_real'])):\n",
    "    predict_dynamic_series = to_pandas_extend(predict_entry, 'feat_dynamic_real', i)\n",
    "    predict_dynamic_series.plot()\n",
    "plt.grid(which=\"both\")\n",
    "plt.legend([\"predict series\", \"end of train series\", \"end of test series\"]+['dynamic_feat_'+str(i) for i in range(len(predict_entry['feat_dynamic_real']))], loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = {}\n",
    "predictors = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimator Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.model.canonical import CanonicalRNNEstimator\n",
    "from gluonts.model.deep_factor import DeepFactorEstimator\n",
    "from gluonts.model.deepar import DeepAREstimator\n",
    "from gluonts.model.deepstate import DeepStateEstimator\n",
    "from gluonts.model.deepvar import DeepVAREstimator\n",
    "from gluonts.model.gp_forecaster import GaussianProcessEstimator\n",
    "from gluonts.model.gpvar import GPVAREstimator\n",
    "from gluonts.model.lstnet import LSTNetEstimator\n",
    "from gluonts.model.n_beats import NBEATSEstimator\n",
    "from gluonts.model.seq2seq import MQCNNEstimator, MQRNNEstimator, RNN2QRForecaster, Seq2SeqEstimator\n",
    "from gluonts.model.simple_feedforward import SimpleFeedForwardEstimator\n",
    "from gluonts.model.transformer import TransformerEstimator\n",
    "from gluonts.model.wavenet import WaveNetEstimator\n",
    "\n",
    "from gluonts.block.quantile_output import QuantileOutput\n",
    "from gluonts.trainer import Trainer\n",
    "from gluonts.block.encoder import Seq2SeqEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = CanonicalRNNEstimator(\n",
    "    freq=freq,\n",
    "    prediction_length=prediction_length,\n",
    "    context_length=context_length,\n",
    "    trainer=Trainer(ctx=\"cpu\",\n",
    "                    epochs=200,\n",
    "                    learning_rate=1e-3,\n",
    "                    batch_size=32,\n",
    "                    num_batches_per_epoch=100\n",
    "                   ),\n",
    ")\n",
    "estimators['CanonicalRNN'] = estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = DeepFactorEstimator(\n",
    "    freq=freq,\n",
    "    prediction_length=prediction_length,\n",
    "    context_length=context_length,\n",
    "    trainer=Trainer(ctx=\"cpu\",\n",
    "                    epochs=200,\n",
    "                    learning_rate=1e-3,\n",
    "                    batch_size=32,\n",
    "                    num_batches_per_epoch=100\n",
    "                   ),\n",
    ")\n",
    "estimators['DeepFactor'] = estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = DeepAREstimator(\n",
    "    freq=freq,\n",
    "    prediction_length=prediction_length,\n",
    "    context_length=context_length,\n",
    "    trainer=Trainer(ctx=\"cpu\",\n",
    "                    epochs=200,\n",
    "                    learning_rate=1e-3,\n",
    "                    batch_size=32,\n",
    "                    num_batches_per_epoch=100\n",
    "                   ),\n",
    "    use_feat_dynamic_real=True,  # True\n",
    "    use_feat_static_cat=True,  # True\n",
    "#     cardinality=[61]\n",
    "    cardinality=[17]\n",
    ")\n",
    "estimators['DeepAR'] = estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = DeepStateEstimator(\n",
    "    freq=freq,\n",
    "    prediction_length=prediction_length,\n",
    "    trainer=Trainer(ctx=\"cpu\",\n",
    "                    epochs=200,\n",
    "                    learning_rate=1e-3,\n",
    "                    batch_size=32,\n",
    "                    num_batches_per_epoch=100\n",
    "                   ),\n",
    "    use_feat_dynamic_real=True,  # True\n",
    "    use_feat_static_cat=True,  # True\n",
    "#     cardinality=[61]\n",
    "    cardinality=[17]\n",
    ")\n",
    "estimators['DeepState'] = estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = DeepVAREstimator(  # use multi\n",
    "    freq=freq,\n",
    "    prediction_length=prediction_length,\n",
    "    context_length=context_length,\n",
    "    trainer=Trainer(ctx=\"cpu\",\n",
    "                    epochs=200,\n",
    "                    learning_rate=1e-3,\n",
    "                    batch_size=32,\n",
    "                    num_batches_per_epoch=100\n",
    "                   ),\n",
    "    target_dim=96\n",
    ")\n",
    "estimators['DeepVAR'] = estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = GaussianProcessEstimator(\n",
    "    freq=freq,\n",
    "    prediction_length=prediction_length,\n",
    "    context_length=context_length,\n",
    "    trainer=Trainer(ctx=\"cpu\",\n",
    "                    epochs=200,\n",
    "                    learning_rate=1e-3,\n",
    "                    batch_size=32,\n",
    "                    num_batches_per_epoch=100\n",
    "                   ),\n",
    "    #     cardinality=61\n",
    "    cardinality=17\n",
    ")\n",
    "estimators['GaussianProcess'] = estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = GPVAREstimator(  # use multi\n",
    "    freq=freq,\n",
    "    prediction_length=prediction_length,\n",
    "    context_length=context_length,\n",
    "    trainer=Trainer(ctx=\"cpu\",\n",
    "                    epochs=200,\n",
    "                    learning_rate=1e-3,\n",
    "                    batch_size=32,\n",
    "                    num_batches_per_epoch=100\n",
    "                   ),\n",
    "    target_dim=96\n",
    ")\n",
    "estimators['GPVAR'] = estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = LSTNetEstimator(  # use multi\n",
    "    freq=freq,\n",
    "    prediction_length=prediction_length,\n",
    "    context_length=context_length,\n",
    "    num_series=96,\n",
    "    skip_size=4,\n",
    "    ar_window=4,\n",
    "    channels=72,\n",
    "    trainer=Trainer(ctx=\"cpu\",\n",
    "                    epochs=200,\n",
    "                    learning_rate=1e-3,\n",
    "                    batch_size=32,\n",
    "                    num_batches_per_epoch=100\n",
    "                   ),\n",
    ")\n",
    "estimators['LSTNet'] = estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = NBEATSEstimator(\n",
    "    freq=freq,\n",
    "    prediction_length=prediction_length,\n",
    "    context_length=context_length,\n",
    "    trainer=Trainer(ctx=\"cpu\",\n",
    "                    epochs=200,\n",
    "                    learning_rate=1e-3,\n",
    "                    batch_size=32,\n",
    "                    num_batches_per_epoch=100\n",
    "                   ),\n",
    ")\n",
    "estimators['NBEATS'] = estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = MQCNNEstimator(\n",
    "    freq=freq,\n",
    "    prediction_length=prediction_length,\n",
    "    context_length=context_length,\n",
    "    trainer=Trainer(ctx=\"cpu\",\n",
    "                    epochs=200,\n",
    "                    learning_rate=1e-3,\n",
    "                    batch_size=32,\n",
    "                    num_batches_per_epoch=100\n",
    "                   ),\n",
    ")\n",
    "estimators['MQCNN'] = estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = MQRNNEstimator(\n",
    "    freq=freq,\n",
    "    prediction_length=prediction_length,\n",
    "    context_length=context_length,\n",
    "    trainer=Trainer(ctx=\"cpu\",\n",
    "                    epochs=200,\n",
    "                    learning_rate=1e-3,\n",
    "                    batch_size=32,\n",
    "                    num_batches_per_epoch=100\n",
    "                   ),\n",
    ")\n",
    "estimators['MQRNN'] = estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO\n",
    "# estimator = RNN2QRForecaster(\n",
    "#     freq=freq,\n",
    "#     prediction_length=prediction_length,\n",
    "#     context_length=context_length,\n",
    "#     trainer=Trainer(ctx=\"cpu\",\n",
    "#                     epochs=200,\n",
    "#                     learning_rate=1e-3,\n",
    "#                     batch_size=32,\n",
    "#                     num_batches_per_epoch=100\n",
    "#                    ),\n",
    "# #     cardinality=[61]\n",
    "#     cardinality=[17],\n",
    "#     embedding_dimension=4,\n",
    "#     encoder_rnn_layer=4,\n",
    "#     encoder_rnn_num_hidden=4,\n",
    "#     decoder_mlp_layer=[4],\n",
    "#     decoder_mlp_static_dim=4\n",
    "# )\n",
    "# estimators['RNN2QR'] = estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO\n",
    "# estimator = Seq2SeqEstimator(\n",
    "#     freq=freq,\n",
    "#     prediction_length=prediction_length,\n",
    "#     context_length=context_length,\n",
    "#     trainer=Trainer(ctx=\"cpu\",\n",
    "#                     epochs=200,\n",
    "#                     learning_rate=1e-3,\n",
    "#                     batch_size=32,\n",
    "#                     num_batches_per_epoch=100\n",
    "#                    ),\n",
    "# #     cardinality=[61]\n",
    "#     cardinality=[17],\n",
    "#     embedding_dimension=4,\n",
    "#     encoder=Seq2SeqEncoder(),\n",
    "#     decoder_mlp_layer=[4],\n",
    "#     decoder_mlp_static_dim=4\n",
    "# )\n",
    "# estimators['Seq2Seq'] = estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = SimpleFeedForwardEstimator(\n",
    "    num_hidden_dimensions=[40, 40],\n",
    "    prediction_length=prediction_length,\n",
    "    context_length=context_length,\n",
    "    freq=freq,\n",
    "    trainer=Trainer(ctx=\"cpu\",\n",
    "                    epochs=200,\n",
    "                    learning_rate=1e-3,\n",
    "                    batch_size=32,\n",
    "                    num_batches_per_epoch=100\n",
    "                   )\n",
    ")\n",
    "estimators['SimpleFeedForward'] = estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = TransformerEstimator(\n",
    "    freq=freq,\n",
    "    prediction_length=prediction_length,\n",
    "    trainer=Trainer(ctx=\"cpu\",\n",
    "                    epochs=200,\n",
    "                    learning_rate=1e-3,\n",
    "                    batch_size=32,\n",
    "                    num_batches_per_epoch=100\n",
    "                   ),\n",
    "#     cardinality=[61]\n",
    "    cardinality=[17]\n",
    ")\n",
    "estimators['Transformer'] = estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = WaveNetEstimator(\n",
    "    freq=freq,\n",
    "    prediction_length=prediction_length,\n",
    "    trainer=Trainer(ctx=\"cpu\",\n",
    "                    epochs=200,\n",
    "                    learning_rate=1e-3,\n",
    "                    batch_size=32,\n",
    "                    num_batches_per_epoch=100\n",
    "                   ),\n",
    "#     cardinality=[61]\n",
    "    cardinality=[17]\n",
    ")\n",
    "estimators['WaveNet'] = estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for name, estimator in estimators.items():\n",
    "    start = time.time()\n",
    "    try:\n",
    "        predictor1 = estimator.train(train_ds)\n",
    "    except:\n",
    "        predictor1 = estimator.train(train_ds_multi)\n",
    "    predictors[name] = predictor1\n",
    "    end = time.time()\n",
    "    print(name, end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p gluonts_model/deepar/\n",
    "\n",
    "# save the trained model in tmp/\n",
    "predictor1.serialize(Path(\"gluonts_model/deepar/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loads it back\n",
    "from gluonts.model.predictor import Predictor\n",
    "predictor1 = Predictor.deserialize(Path(\"gluonts_model/deepar/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictor Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.model.naive_2 import Naive2Predictor\n",
    "from gluonts.model.npts import NPTSPredictor\n",
    "from gluonts.model.prophet import ProphetPredictor\n",
    "from gluonts.model.r_forecast import RForecastPredictor\n",
    "from gluonts.model.seasonal_naive import SeasonalNaivePredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # TODO Multiplicative seasonality is not appropriate for zero and negative values\n",
    "# predictor2 = Naive2Predictor(freq=freq, prediction_length=prediction_length, season_length=context_length)\n",
    "# predictors['Naive2'] = predictor2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "predictor2 = NPTSPredictor(freq=freq, prediction_length=prediction_length, context_length=context_length)\n",
    "predictors['NPTS'] = predictor2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def configure_model(model):\n",
    "    model.add_seasonality(\n",
    "        name='weekly', period=7, fourier_order=3, prior_scale=0.1\n",
    "    )\n",
    "    return model\n",
    "\n",
    "predictor2 = ProphetPredictor(freq=freq,\n",
    "                              prediction_length=prediction_length,\n",
    "                              init_model=configure_model)\n",
    "predictors['Prophet'] = predictor2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "predictor2 = RForecastPredictor(freq=freq,\n",
    "                              prediction_length=prediction_length,\n",
    "                              method_name='arima',  # The method from rforecast to be used one of “ets”, “arima”, “tbats” (bug), “croston” (bug), “mlp” (bug).\n",
    "                              period=context_length,\n",
    "                              trunc_length=len(train[0]['target']))\n",
    "predictors['ARIMA'] = predictor2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "predictor2 = SeasonalNaivePredictor(freq=freq, prediction_length=prediction_length)\n",
    "predictors['SeasonalNaive'] = predictor2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select a predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictor = predictor1\n",
    "# predictor = predictor2\n",
    "predictor = predictors['SeasonalNaive']  # DeepAR or SeasonalNaive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from gluonts.evaluation.backtest import make_evaluation_predictions\n",
    "\n",
    "forecast_it, ts_it = make_evaluation_predictions(\n",
    "    dataset=predict_ds,  # test dataset\n",
    "    predictor=predictor,  # predictor\n",
    "    num_samples=100,  # number of sample paths we want for evaluation\n",
    ")\n",
    "\n",
    "forecasts = list(forecast_it)\n",
    "tss = list(ts_it)\n",
    "print(len(forecasts), len(tss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from gluonts.evaluation import Evaluator\n",
    "\n",
    "evaluator = Evaluator(quantiles=[0.1, 0.5, 0.9])\n",
    "agg_metrics, item_metrics = evaluator(iter(tss), iter(forecasts), num_series=len(predict_ds))\n",
    "\n",
    "print(json.dumps(agg_metrics, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_metrics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_metrics.plot(x='MSIS', y='MASE', kind='scatter')\n",
    "plt.grid(which=\"both\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_metrics['sMAPE'].plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quartiles = pd.cut(item_metrics.sMAPE, [0, .1, .2, .3, .4, .5, .6, .7, .8, .9, 1, 2])\n",
    "#print(quartiles)\n",
    "def get_stats(group):\n",
    "    return {'sMAPE': group.count()}\n",
    "grouped = item_metrics.sMAPE.groupby(quartiles)\n",
    "price_bucket_amount = grouped.apply(get_stats).unstack()\n",
    "#price_bucket_amount\n",
    "price_bucket_amount.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_metrics['sMAPE'].idxmin(), item_metrics['sMAPE'].min(), item_metrics['sMAPE'].idxmax(), item_metrics['sMAPE'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize a Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prob_forecasts(ts_entry, forecast_entry):\n",
    "#     print('ts_entry:', ts_entry)\n",
    "#     print('forecast_entry:', forecast_entry)\n",
    "    plot_length = context_length+prediction_length\n",
    "    prediction_intervals = (50.0, 90.0)\n",
    "#     prediction_intervals = [80.0]\n",
    "    legend = [\"observations\", \"median prediction\"] + [f\"{k}% prediction interval\" for k in prediction_intervals][::-1]\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 7))\n",
    "    pd.plotting.register_matplotlib_converters()  # https://stackoverflow.com/questions/43206554/typeerror-float-argument-must-be-a-string-or-a-number-not-period\n",
    "    ts_entry[-plot_length:].plot(ax=ax)  # plot the time series\n",
    "    forecast_entry.plot(prediction_intervals=prediction_intervals, color='orange')\n",
    "    plt.grid(which=\"both\")\n",
    "    plt.legend(legend, loc=\"upper left\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_metrics(customer_id=0, target_quantile=0.5, plot_graph=True):\n",
    "\n",
    "    # first entry of the time series list\n",
    "    ts_entry = tss[customer_id]\n",
    "\n",
    "    # first 5 values of the time series (convert from pandas to numpy)\n",
    "    # print(np.array(ts_entry[:5]).reshape(-1,))\n",
    "\n",
    "    # first entry of dataset.test\n",
    "    dataset_test_entry = next(iter(predict_ds))\n",
    "\n",
    "    # first 5 values\n",
    "    # print(dataset_test_entry['target'][:5])\n",
    "\n",
    "    # first entry of the forecast list\n",
    "    forecast_entry = forecasts[customer_id]\n",
    "\n",
    "    if plot_graph:\n",
    "        print(f\"Number of sample paths: {forecast_entry.num_samples}\")\n",
    "        print(f\"Dimension of samples: {forecast_entry.samples.shape}\")\n",
    "        print(f\"Start date of the forecast window: {forecast_entry.start_date}\")\n",
    "        print(f\"Frequency of the time series: {forecast_entry.freq}\")\n",
    "\n",
    "        print(f\"Mean of the future window:\\n {forecast_entry.mean}\")\n",
    "        print(f\"0.5-quantile (median) of the future window:\\n {forecast_entry.quantile(0.5)}\")\n",
    "        print(f\"target_value:\\n {ts_entry[-prediction_length:].values.reshape((1, -1))}\")\n",
    "\n",
    "        plot_prob_forecasts(ts_entry, forecast_entry)\n",
    "    \n",
    "    y_label = list(ts_entry[-prediction_length:].values.reshape((1, -1))[0])\n",
    "#     y_pred = list(forecast_entry.mean)\n",
    "    y_pred = list(forecast_entry.quantile(target_quantile))\n",
    "    return y_label, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "target_quantile=0.7\n",
    "y_labels = []\n",
    "y_preds = []\n",
    "\n",
    "for i in range(len(tss)):\n",
    "    y_label, y_pred = show_metrics(i, target_quantile=target_quantile, plot_graph=True)\n",
    "    y_labels.append(y_label)\n",
    "    y_preds.append(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def smape(a, f):\n",
    "    return 1/len(a) * np.sum(2 * np.abs(f-a) / (np.abs(a) + np.abs(f))*100)/100\n",
    "\n",
    "def eval_metric(a, f):\n",
    "#     print('a:', a)\n",
    "#     print('f:', f)\n",
    "    new_a = []\n",
    "    new_f = []\n",
    "    for i in range(len(a)):\n",
    "        if a[i] != 0:\n",
    "            new_a.append(a[i])\n",
    "            new_f.append(f[i])\n",
    "    new_a = np.array(a)\n",
    "    new_f = np.array(f)\n",
    "#     print('new_a:', new_a)\n",
    "#     print('new_f:', new_f)\n",
    "    return np.mean((new_f-new_a)/new_a)\n",
    "\n",
    "def eval_metric2(a, f):\n",
    "#     print('a:', a)\n",
    "#     print('f:', f)\n",
    "    new_a = []\n",
    "    new_f = []\n",
    "    for i in range(len(a)):\n",
    "        if a[i] != 0:\n",
    "            new_a.append(a[i])\n",
    "            new_f.append(f[i])\n",
    "    new_a = np.array(a)\n",
    "    new_f = np.array(f)\n",
    "#     print('new_a:', new_a)\n",
    "#     print('new_f:', new_f)\n",
    "    return np.mean(np.abs(new_f-new_a)/np.abs(new_a))\n",
    "\n",
    "print('y_labels:', len(y_labels))\n",
    "print('y_preds:', len(y_preds))\n",
    "y_labels = np.array(y_labels)\n",
    "y_preds = np.array(y_preds)\n",
    "print(\"RMSE:\",np.sqrt(metrics.mean_squared_error(y_labels, y_preds)))\n",
    "print(\"MAE:\",metrics.mean_absolute_error(y_labels, y_preds))\n",
    "print(\"Target Mean:\",y_labels.mean())\n",
    "smape_score = 0\n",
    "smapes = []\n",
    "rmses = []\n",
    "maes = []\n",
    "means = []\n",
    "for i in range(y_labels.shape[0]):\n",
    "    smapei = smape(y_labels[i], y_preds[i])\n",
    "    rmsei = np.sqrt(metrics.mean_squared_error(y_labels[i], y_preds[i]))\n",
    "    maei = metrics.mean_absolute_error(y_labels[i], y_preds[i])\n",
    "    smape_score += smapei\n",
    "    smapes.append(smapei)\n",
    "    rmses.append(rmsei)\n",
    "    maes.append(maei)\n",
    "    means.append(y_labels[i].mean())\n",
    "print(\"sMAPE:\",smape_score/y_labels.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, predictor in predictors.items():\n",
    "    print(name)\n",
    "    \n",
    "    forecast_it, ts_it = make_evaluation_predictions(\n",
    "        dataset=predict_ds,  # test dataset\n",
    "        predictor=predictor,  # predictor\n",
    "        num_samples=100,  # number of sample paths we want for evaluation\n",
    "    )\n",
    "\n",
    "    forecasts = list(forecast_it)\n",
    "    tss = list(ts_it)\n",
    "    print(len(forecasts), len(tss))\n",
    "    \n",
    "    target_quantile = 0.7\n",
    "    y_labels = []\n",
    "    y_preds = []\n",
    "\n",
    "    for i in range(len(tss)):\n",
    "        y_label, y_pred = show_metrics(i, target_quantile=target_quantile, plot_graph=False)\n",
    "        y_labels.append(y_label)\n",
    "        y_preds.append(y_pred)\n",
    "        \n",
    "    y_sum_labels = np.sum(y_labels, axis=0)\n",
    "    y_sum_preds = np.sum(y_preds, axis=0)\n",
    "    print(\"RMSE:\",np.sqrt(metrics.mean_squared_error(y_sum_labels, y_sum_preds)))\n",
    "    print(\"MAE:\",metrics.mean_absolute_error(y_sum_labels, y_sum_preds))\n",
    "    print(\"SMAPE:\", smape(y_sum_labels, y_sum_preds))\n",
    "    print(\"EVAL METRIC:\", eval_metric(y_sum_labels, y_sum_preds))\n",
    "    print(\"EVAL METRIC2:\", eval_metric2(y_sum_labels, y_sum_preds))\n",
    "    print(\"Target Mean:\",y_sum_labels.mean())\n",
    "    \n",
    "    plt.plot(y_sum_labels,\"b-\",label=\"label\")\n",
    "    plt.plot(y_sum_preds,\"r-\",label=\"predict\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
